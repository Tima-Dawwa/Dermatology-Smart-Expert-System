# llm.py

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# ======== CONFIG ========
MODEL_PATH = "AI/DialoGPT-medium"

# ======== LOAD MODEL ========
print("Loading model...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)

generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device=0 if model.device.type == "cuda" else -1,
)

print("Model loaded successfully.")

# ======== INFERENCE FUNCTION ========


def explain_result_with_llm(result_text: str) -> str:
    """
    Generate a user-friendly explanation for the given result.

    Args:
        result_text (str): The raw output from the expert system.

    Returns:
        str: A simplified explanation generated by the LLM.
    """
    prompt = f"Explain this diagnosis in simple terms for a non-expert:\n{result_text}\nExplanation:"

    try:
        outputs = generator(
            prompt,
            max_length=200,
            temperature=0.7,
            do_sample=True,
            top_p=0.9,
            num_return_sequences=1
        )

        generated = outputs[0]['generated_text']
        # Remove the prompt from the output
        explanation = generated.replace(prompt, '').strip()
        return explanation

    except Exception as e:
        print(f"Error during generation: {e}")
        return "Sorry, I couldn't generate an explanation."


# ======== TEST (optional) ========
if __name__ == "__main__":
    test_result = """
ðŸ“‹ Primary Diagnosis: Lipoma
    Confidence: 78.8%
    Reasoning: Soft lump is characteristic of lipoma.
        """
    explanation = explain_result_with_llm(test_result)
    print("\nGenerated Explanation:\n", explanation)
