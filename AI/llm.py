import os
from huggingface_hub import InferenceClient





def explain_result_with_llm(result_text: str) -> str:
    """
    Generate a user-friendly explanation for the given result by calling DeepSeek-R1 model
    via Hugging Face Inference API.

    Args:
        result_text (str): The raw output from the expert system.

    Returns:
        str: A simplified explanation generated by the LLM.
    """
    prompt = f"Explain this diagnosis in simple terms for a non-expert:\n{result_text}\nExplanation:"

    try:
        completion = client.chat.completions.create(
            model="deepseek-ai/DeepSeek-R1",
            messages=[
                {"role": "user", "content": prompt}
            ],
            max_tokens=200,
            temperature=0.7,
            top_p=0.9,
            n=1,
        )
        explanation = completion.choices[0].message.content.strip()
        return explanation

    except Exception as e:
        print(f"Error during generation: {e}")
        return "Sorry, I couldn't generate an explanation."


# ======== TEST (optional) ========
if __name__ == "__main__":
    test_result = """
ðŸ“‹ Primary Diagnosis: Lipoma
    Confidence: 78.8%
    Reasoning: Soft lump is characteristic of lipoma.
    """
    explanation = explain_result_with_llm(test_result)
    print("\nGenerated Explanation:\n", explanation)
